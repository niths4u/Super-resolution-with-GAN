{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from math import log10\n",
    "\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.utils as utils\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pytorch_ssim\n",
    "from data_utils import TrainDatasetFromFolder, ValDatasetFromFolder, display_transform\n",
    "from torchvision.transforms import Compose, RandomCrop, ToTensor, ToPILImage, CenterCrop, Resize\n",
    "from torchvision import transforms\n",
    "from loss import GeneratorLoss\n",
    "from model import Generator, Discriminator\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CROP_SIZE = 12\n",
    "UPSCALE_FACTOR = 4\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datalocation = os.path.join('C:', os.sep, 'data_science_tasks', 'notebooks','masters','code1','SRforOCR','RELEASE_2015-08-31','DATA','TRAIN','HD')\n",
    "val_datalocation = os.path.join('C:', os.sep, 'data_science_tasks', 'notebooks','masters','code1','SRforOCR','RELEASE_2015-08-31','DATA','VAL','HD')\n",
    "train_set = TrainDatasetFromFolder(train_datalocation, crop_size=CROP_SIZE, upscale_factor=UPSCALE_FACTOR)\n",
    "val_set = ValDatasetFromFolder(val_datalocation, upscale_factor=UPSCALE_FACTOR)\n",
    "train_loader = DataLoader(dataset=train_set, num_workers=1, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_set, num_workers=1, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# generator parameters: 734219\n",
      "# discriminator parameters: 5215425\n"
     ]
    }
   ],
   "source": [
    "modelG = Generator(UPSCALE_FACTOR)\n",
    "print('# generator parameters:', sum(param.numel() for param in modelG.parameters()))\n",
    "modelD = Discriminator()\n",
    "print('# discriminator parameters:', sum(param.numel() for param in modelD.parameters()))\n",
    "\n",
    "generator_criterion = GeneratorLoss()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    modelG.cuda()\n",
    "    modelD.cuda()\n",
    "    generator_criterion.cuda()\n",
    "\n",
    "optimizerG = optim.Adam(modelG.parameters())\n",
    "optimizerD = optim.Adam(modelD.parameters())\n",
    "\n",
    "results = {'d_loss': [], 'g_loss': [], 'd_score': [], 'g_score': [], 'psnr': [], 'ssim': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1/3] Loss_D: 0.9885 Loss_G: -4.6736 D(x): 0.9645 D(G(z)): 0.9530: 100%|██████████| 567/567 [01:45<00:00,  5.40it/s]\n",
      "[converting LR images to SR images] PSNR: 20.8817 dB SSIM: 0.8479: 100%|██████████| 21/21 [00:39<00:00,  1.86s/it]\n",
      "[2/3] Loss_D: 1.0000 Loss_G: -5.2005 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 567/567 [01:43<00:00,  5.47it/s]\n",
      "[converting LR images to SR images] PSNR: 21.2742 dB SSIM: 0.8699: 100%|██████████| 21/21 [00:37<00:00,  1.78s/it]\n",
      "[3/3] Loss_D: 1.0000 Loss_G: -5.5755 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 567/567 [01:43<00:00,  5.46it/s]\n",
      "[converting LR images to SR images] PSNR: 22.4313 dB SSIM: 0.8848: 100%|██████████| 21/21 [00:36<00:00,  1.75s/it]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_bar = tqdm(train_loader)\n",
    "    running_results = {'batch_sizes': 0, 'd_loss': 0, 'g_loss': 0, 'd_score': 0, 'g_score': 0}\n",
    "\n",
    "    modelG.train()\n",
    "    modelD.train()\n",
    "    for data, target in train_bar:\n",
    "        # print(data.size())\n",
    "        g_update_first = True\n",
    "        #batch_size = data.size(0)\n",
    "        batch_size = 50\n",
    "        running_results['batch_sizes'] += batch_size\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize D(x)-1-D(G(z))\n",
    "        ###########################\n",
    "        real_img = Variable(target)\n",
    "        if torch.cuda.is_available():\n",
    "            real_img = real_img.cuda()\n",
    "        z = Variable(data)\n",
    "        if torch.cuda.is_available():\n",
    "            z = z.cuda()\n",
    "        fake_img = modelG(z)\n",
    "\n",
    "        modelD.zero_grad()\n",
    "        real_out = modelD(real_img).mean()\n",
    "        fake_out = modelD(fake_img).mean()\n",
    "        d_loss = 1 - real_out + fake_out\n",
    "        d_loss.backward(retain_graph=True)\n",
    "        \n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: minimize 1-D(G(z)) + Perception Loss + Image Loss + TV Loss\n",
    "        ###########################\n",
    "        modelG.zero_grad()\n",
    "        g_loss = generator_criterion(fake_out, fake_img, real_img)\n",
    "        g_loss.backward()\n",
    "        \n",
    "        fake_img = modelG(z)\n",
    "        fake_out = modelD(fake_img).mean()\n",
    "        \n",
    "        optimizerD.step()\n",
    "        optimizerG.step()\n",
    "\n",
    "        # loss for current batch before optimization \n",
    "        running_results['g_loss'] += g_loss.item() * batch_size\n",
    "        running_results['d_loss'] += d_loss.item() * batch_size\n",
    "        running_results['d_score'] += real_out.item() * batch_size\n",
    "        running_results['g_score'] += fake_out.item() * batch_size\n",
    "\n",
    "        train_bar.set_description(desc='[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f' % (\n",
    "            epoch, NUM_EPOCHS, running_results['d_loss'] / running_results['batch_sizes'],\n",
    "            running_results['g_loss'] / running_results['batch_sizes'],\n",
    "            running_results['d_score'] / running_results['batch_sizes'],\n",
    "            running_results['g_score'] / running_results['batch_sizes']))\n",
    "\n",
    "    modelG.eval()\n",
    "    out_path = 'training_results/SRF_' + str(UPSCALE_FACTOR) + '/'\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_bar = tqdm(val_loader)\n",
    "        valing_results = {'mse': 0, 'ssims': 0, 'psnr': 0, 'ssim': 0, 'batch_sizes': 0}\n",
    "        val_images = []\n",
    "        index = 1\n",
    "        for val_lr, val_hr_restore, val_hr in val_bar:\n",
    "            batch_size = val_lr.size(0)\n",
    "            valing_results['batch_sizes'] += batch_size\n",
    "            lr = val_lr\n",
    "            # recursive padding\n",
    "            for i in range(10):\n",
    "                lr = torch.nn.functional.pad(lr, (1, 1, 1, 1), 'constant', 0)\n",
    "                c, w, h = lr[0].size()\n",
    "                for channel in range(c):\n",
    "                    for j in range(w):\n",
    "                        temp = lr[0, channel, max(1, j-2):min(j+3, w-2), 1:3]\n",
    "                        \n",
    "                        temp.contiguous().view(1, -1)\n",
    "                        # print(temp)\n",
    "                        lr[0, channel, j, 0] = torch.mean(temp)\n",
    "                        \n",
    "                        temp = lr[0, channel, max(1, j-2):min(j+3, w-2), h-4:h-2]\n",
    "                        temp.contiguous().view(1, -1)\n",
    "                        lr[0, channel, j, h-1] = torch.mean(temp)\n",
    "                    for k in range(h):\n",
    "                        temp = lr[0, channel, 1:3, max(1, k-2):min(k+3, h-2)]\n",
    "                        temp.contiguous().view(1, -1)\n",
    "                        lr[0, channel, 0, k] = torch.mean(temp)\n",
    "\n",
    "                        temp = lr[0, channel, w-4:w-2, max(1, k-2):min(k+3, h-2)]\n",
    "                        temp.contiguous().view(1, -1)\n",
    "                        lr[0, channel, w-1, k] = torch.mean(temp)\n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "            hr = val_hr\n",
    "            if torch.cuda.is_available():\n",
    "                lr = lr.cuda()\n",
    "                hr = hr.cuda()\n",
    "            sr = modelG(lr)\n",
    "\n",
    "            sr = sr[:, :, :, 40:]\n",
    "            sr = sr[:, :, :, :-40]\n",
    "            sr = sr[:, :, 40:, :]\n",
    "            sr = sr[:, :, :-40, :]\n",
    "    \n",
    "            batch_mse = ((sr - hr) ** 2).data.mean()\n",
    "            valing_results['mse'] += batch_mse * batch_size\n",
    "            batch_ssim = pytorch_ssim.ssim(sr, hr).item()\n",
    "            valing_results['ssims'] += batch_ssim * batch_size\n",
    "            valing_results['psnr'] = 10 * log10(1 / (valing_results['mse'] / valing_results['batch_sizes']))\n",
    "            valing_results['ssim'] = valing_results['ssims'] / valing_results['batch_sizes']\n",
    "            val_bar.set_description(\n",
    "                desc='[converting LR images to SR images] PSNR: %.4f dB SSIM: %.4f' % (\n",
    "                    valing_results['psnr'], valing_results['ssim']))\n",
    "            gc.collect()\n",
    "            image1 = utils.make_grid(display_transform()(val_hr_restore.squeeze(0)), nrow=3, padding=5)\n",
    "            # image1.ï¼Œ\n",
    "            # print(image1.size())\n",
    "            utils.save_image(image1, out_path + 'epoch_%d_index_%d_hr_restore.png' % (epoch, index), padding=5)\n",
    "\n",
    "            image2 = utils.make_grid(display_transform()(hr.data.cpu().squeeze(0)), nrow=3, padding=5)\n",
    "            # print(image2.size())\n",
    "            utils.save_image(image2, out_path + 'epoch_%d_index_%d_hr.png' % (epoch, index), padding=5)\n",
    "\n",
    "            image3 = utils.make_grid(display_transform()(sr.data.cpu().squeeze(0)), nrow=3, padding=5)\n",
    "            \n",
    "            utils.save_image(image3, out_path + 'epoch_%d_index_%d_sr.png' % (epoch, index), padding=5)\n",
    "            index += 1\n",
    "\n",
    "    \n",
    "            # val_images.extend(\n",
    "            #     [display_transform()(val_hr_restore.squeeze(0)), display_transform()(hr.data.cpu().squeeze(0)),\n",
    "            #      display_transform()(sr.data.cpu().squeeze(0))])\n",
    "\n",
    "        \n",
    "        # val_images = torch.stack(val_images)\n",
    "        # val_images = torch.chunk(val_images, val_images.size(0) // 3)\n",
    "        # val_save_bar = tqdm(val_images, desc='[saving training results]')\n",
    "        # index = 1\n",
    "        # for image in val_save_bar:\n",
    "        #     image = utils.make_grid(image, nrow=3, padding=5)\n",
    "        #     utils.save_image(image, out_path + 'epoch_%d_index_%d.png' % (epoch, index), padding=5)\n",
    "        #     index += 1\n",
    "\n",
    "    # save model parameters\n",
    "    \n",
    "    torch.save(modelG.state_dict(), 'epochs/modelG_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))\n",
    "    torch.save(modelD.state_dict(), 'epochs/modelD_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))\n",
    "    # save loss\\scores\\psnr\\ssim\n",
    "    results['d_loss'].append(running_results['d_loss'] / running_results['batch_sizes'])\n",
    "    results['g_loss'].append(running_results['g_loss'] / running_results['batch_sizes'])\n",
    "    results['d_score'].append(running_results['d_score'] / running_results['batch_sizes'])\n",
    "    results['g_score'].append(running_results['g_score'] / running_results['batch_sizes'])\n",
    "    results['psnr'].append(valing_results['psnr'])\n",
    "    results['ssim'].append(valing_results['ssim'])\n",
    "\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        out_path = 'statistics/'\n",
    "        data_frame = pd.DataFrame(\n",
    "            data={'Loss_D': results['d_loss'], 'Loss_G': results['g_loss'], 'Score_D': results['d_score'],\n",
    "                  'Score_G': results['g_score'], 'PSNR': results['psnr'], 'SSIM': results['ssim']},\n",
    "            index=range(1, epoch + 1))\n",
    "        data_frame.to_csv(out_path + 'srf_' + str(UPSCALE_FACTOR) + '_train_results.csv', index_label='Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
